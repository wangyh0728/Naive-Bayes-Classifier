package code;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.PrintStream;
import java.net.URI;
import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.PriorityQueue;
import java.util.Set;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import format.StringDoubleList;
import format.StringDoubleList.StringDouble;
/**
 * @author Hadoop Group 15
 * This class is to calculate the log likelihood of each profession of a given person according 
 * to the profession lemma frequencies used in training, and predict the top three professions of
 * that person.
 */
public class ClassifyMapred {
	/**
	 * ClassifyMapper class is to map each person to his article lemmas and corresponding lemma frequencies in the
	 * profession lemma index, and calculate log likelihood of each profession
	 */
	public static class ClassifyMapper extends Mapper<LongWritable, Text, Text, Text> {
		//store test people names
		public static Set<String> people = new HashSet<String>();
		//store profession_lemma_index
		public static Map<String, HashMap<String, Integer>> professionLemmaIndex =
				new HashMap<String, HashMap<String, Integer>>();

		/**
		 * read the testing people from the given test file and store it in the people set
		 * read the profession lemma index file generated by the TrainingMapred and store it in
		 * the professionLemmaIndex hash map
		 */
		protected void setup(
				Mapper<LongWritable, Text, Text, Text>.Context context) throws IOException, InterruptedException {
			super.setup(context);
			URI[] files = context.getCacheFiles();
//			Path[] files = context.getLocalCacheFiles();
			//read the people_test.txt
			BufferedReader reader1 = new BufferedReader(new FileReader(new File(
					files[0].toString())));

			String currentLine = null;
			while ((currentLine = reader1.readLine()) != null) {
				people.add(currentLine);
			}
			reader1.close();

			//read the profession_lemma_index
			BufferedReader reader2 = new BufferedReader(new FileReader(new File(
					files[1].toString())));

			while ((currentLine = reader2.readLine()) != null) {
				//splits into profession, total article number, lemma indices
				String[] splits = currentLine.split("\t");
				//profession
				String profession = splits[0].trim();

				String[] totalArticles = splits[1].trim().split(":");
				//total article number in this profession
				int totalNum = Integer.parseInt(totalArticles[1].trim());

				String[] lemmaIndices = splits[2].trim().split(">,<");
				//store lemma and its freq in this profession
				HashMap<String, Integer> lemmaFreq = new HashMap<String, Integer>();
				//iterate through lemma, index in the file
				for (String lemmaIndex: lemmaIndices) {
					String tuple = lemmaIndex.replaceAll("[<>]", "");
					//splits into lemma, freq tuple
					String[] pair = tuple.split(",");
					if (pair.length > 1) {
						String lemma = pair[0];
						int freq = Integer.parseInt(pair[1]);
						lemmaFreq.put(lemma, freq);

					}
				}
				//add total number of articles in this profession into the hash map
				lemmaFreq.put("total_article_number", totalNum);
				professionLemmaIndex.put(profession, lemmaFreq);
			}
			reader2.close();
		}

		/**
		 * map lemmas in articles to the profession lemma index and calculate log likelihood of each profession
		 */
		public void map(LongWritable key, Text values, Context context) throws IOException, InterruptedException {
			String[] splits = values.toString().split("\t");
			if (splits.length > 1){
				String peopleName = splits[0].trim();
				String indices = splits[1].trim();
				//if the people name can be found in the people professions hash map
				if (people.contains(peopleName)) {
					//System.out.println(peopleName);
					Map<String, Double> professionValues = new HashMap<String, Double>();
					//iterate through each profession
					for (String profession: professionLemmaIndex.keySet()) {	

						double logValue = 0;
						//split all indices to each lemmaIndex
						String[] lemmaIndices = indices.split(">,<");
						for (String lemmaIndex: lemmaIndices) {
							String tuple = lemmaIndex.replaceAll("[<>]", "");
							//split tuple into lemma and frequency
							String[] pair = tuple.split(",");
							String lemma = pair[0];
							int freq = Integer.parseInt(pair[1]);
							//if the lemma can be found in the profession lemma index
							if (professionLemmaIndex.get(profession).containsKey(lemma))
								//log value = log value + log(P(lemma|profession))
								logValue += freq * Math.log(professionLemmaIndex.get(profession).get(lemma)*1.0 /
										professionLemmaIndex.get(profession).get("total_article_number"));
							//if the lemma cannot be found
							else
								//log value = log value + log(P(0.1|profession))
								logValue += freq * Math.log(0.1 / professionLemmaIndex.get(profession).get("total_article_number"));
						}	
						context.write(new Text (peopleName), new Text(new StringDouble(profession, logValue).toString()));						
					}
				}
			}
		}
	}
	
	/**
	 * ClassifyReducer class is to reduce all the people with their log likelihood value of each profession
	 * and predict the top three most likely professions
	 */
	public static class ClassifyReducer extends Reducer<Text, Text, Text, Text> {

		/**
		 * iterate through all the professions and values of each person, and sort out the top three
		 * professions
		 */
		public void reduce (Text person, Iterable<Text> list, Context context) throws IOException, InterruptedException {
			Map<String, Double> professionResult = new HashMap<String, Double>();
			for (Text x: list) {
				String[] splits = x.toString().split(",");
				professionResult.put(splits[0].trim(), Double.parseDouble(splits[1].trim()));
			}
			
			final Map<String, Double> map = professionResult;
			Comparator<String> comparator = new Comparator<String>() {
				public int compare(String s1, String s2){
					if (map.get(s1) > map.get(s2))
						return -1;
					else if (map.get(s1) < map.get(s2))
						return 1;
					else
						return 0;
				}
			};

			PriorityQueue<String> queue = new PriorityQueue<String>(professionResult.size(), comparator);
			for (String profession: map.keySet()) {
				queue.add(profession);
			}
			String profession1 = queue.remove();
			String profession2 = queue.remove();
			String profession3 = queue.remove();

			String result = ": " + profession1 + ", " + profession2 + ", " + profession3;
			context.write(person, new Text(result));
		}
	}

	/**
	 * run ClassifyMapred class
	 */
	public static void main(String[] args) throws Exception {
		Job job = Job.getInstance();
		job.setJarByClass(ClassifyMapred.class);
		job.setJobName("Classifying");
		job.getConfiguration().set("mapreduce.job.queuename","hadoop15");

		job.setMapperClass(ClassifyMapper.class);
		job.setReducerClass(ClassifyReducer.class);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(Text.class);

		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);

		FileInputFormat.setInputPaths(job, new Path("hdfs://localhost:9000/input"));
		FileOutputFormat.setOutputPath(job, new Path("hdfs://localhost:9000/output2"));
//		FileInputFormat.setInputPaths(job, new Path(args[0]));
//		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		job.addCacheFile(new URI("/Users/Yahui/Desktop/profession_test.txt")); 
		job.addCacheFile(new URI("/Users/Yahui/Desktop/part-r-00000-1"));
//		job.addCacheFile(new URI(args[2])); 
//		job.addCacheFile(new URI(args[3]));

		job.waitForCompletion(true);
	}
}
